## 데이터에서 학습한다
데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 말이다. 

2장의 퍼셉트론도 선형 분리 가능 문제라면 데이터로부터 자동으로 학습할 수 있다. 선형 분리 가능 문제는 유한 번의 학습을 통해 풀 수 있다는 사실이 퍼셉트론 수렴 정리(perceptron convergence theorem)로 증명되었다. 하지만 비성형 분리 문제는 자동으로 학습할 수 없다.

### 데이터 주도 학습
데이터에서 답을 찾고 데이터에서 패턴을 발견하는 것이 기계학습이다. 이처럼 데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있다.

보통 어떤 문제를 해결하고자할 때 사람이 규칙성을 찾아 해결한다. 반면 기계 학습은 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다. 게다가 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 특성을 갖고있다.
![](https://images.velog.io/images/megachj/post/1539a237-b16d-425f-8e35-5f7216bf813c/Internet_20200301_111928.png)

위 그림처럼 사람의 문제 해결 -> 기계학습 -> 딥러닝 으로 올 수록 사람의 개입이 최소화된다.

딥러닝을 종단간 기계학습(end-to-end machine learning)이라고 하는데, 처음부터 끝가지라는 의미로 데이터 입력에서 결과 출력을 사람의 개입 없이 얻는다는 뜻이다.

### 훈련 데이터, 시험 데이터
기계학습 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습과 실험을 수행하는 것이 일반적이다.

데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 그 데이터셋에만 지나치게 최적화된 오버피팅 문제가 발생한다. 이 오버피팅을 피하는게 기계학습의 중요한 과제이기도 하다.

## 손실 함수
> 신경망 학습에서 손실 함수(loss function)라는 지표를 기준으로 `최적의 매개변수 값`을 탐색한다. 손실 함수는 임의의 함수를 사용할 수 있지만 일반적으로 `오차제곱합`과 `교차 엔트로피 오차`를 사용한다.

### 오차제곱합
가장 많이 쓰이는 손실 함수는 오차제곱합(sum of squares for error, SSE)이다.

![](https://images.velog.io/images/megachj/post/07a4b992-c322-403d-9d36-21a6d22fc98d/4-1.png)

여기서 yk는 신경망 출력(추정값)이고, tk는 정답 레이블, k는 데이터의 차원 수를 나타낸다.

MNIST를 예로 보면 yk, tk는 다음과 같은 원소 10개짜리 데이터이다.
```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```
위처럼 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법을 `원-핫 인코딩`이라고 한다.

다음은 파이썬 구현이다.
```python
def sum_squares_error(y, t):
	return 0.5 * np.sum((y-t)**2)
```

### 교차 엔트로피 오차
또 다른 손실 함수로서 교차 엔트로피 오차(cross entropy error, CEE)도 자주 이용한다.

![](https://images.velog.io/images/megachj/post/cbd0b0a3-13cc-4ac2-b23e-ddf4c425d9d9/4-2.png)

동일하게 yk는 신경망 출력(추정값), tk는 정답 레이블이다.  

다음은 파이썬 구현이다.
```python
def cross_entropy_error(y, t):
	delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
log 함수의 0의 우극한값이 -inf 이므로, 아주 작은 값 delta를 더해 0이 되지 않도록 하였다.

### 미니배치 학습
기계학습 문제는 훈련 데이터에 대해 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. 이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다.

앞에서 살펴본 것들은 데이터 1개에 대한 손실 함수였고, 데이터 N개에 대한 전체 손실함수를 생각해야 한다. 교차 엔트로피 오차 함수의 데이터 N개에 대한 수식은 다음과 같다.

![](https://images.velog.io/images/megachj/post/1fd45ce1-ab09-400b-9e7f-e810dacbde13/4-3.png)

모든 데이터에 대해 손실 함수를 계산하면 좋지만 시간이 많이 걸린다. 그래서 전체 데이터중 임의로 표본을 뽑아 표본만을 대상으로 학습해야한다. 이를 미니배치 학습이라 한다.

### 왜 손실 함수를 설정하는가?
모델의 궁극적인 목표는 정확도일텐데, 정확도 지표대신에 왜 손실함수라는 우회적인 방법을 사용하는가?

결론부터 말하면, 정확도를 지표로 사용했을때 미분이 대부분 0이 되기 때문이다.

정확도 지표를 사용한 경우를 보자. 한 신경망이 100장 중 32장을 올바로 인식한다고 하면 정확도는 32%이다. 여기서 매개변수를 약간 조정한다고 해도 정확도는 그대로 32%일 것이다. 개선된다고 하더라도 그 값은 32.01%와 같은 연속적인 변화가 아닌 33%, 34%처럼 불연속적인 값이다. 반면 오차함수는 매개변수에 대해 값이 연속적인 값이다.

이는 활성화 함수로 계단 함수를 사용하지 않는 이유와 같다. 이는 신경망 학습에서 중요한 성질로 기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습할 수 있다.

## 수치 미분
### 미분
![](https://images.velog.io/images/megachj/post/21bde99c-5a89-4bc4-b400-6999ed70d34a/4-4.png)

미분의 정의를 그대로 파이썬으로 구현하기엔 문제가있다. h가 0으로 무한히 가까워지는 것을 표현할 수 없어서, 작은 h를 지정해주어야 하는데 너무 작게 하면 반올림 오차를 일으킨다. 보통 h로 10^-4를 사용하면 좋은 결과를 얻는다고 알려져 있다.

이렇게 h를 작게해 근사 값으로 미분하는 것을 수치 미분이라한다. 좀 더 자세히는 해석적 미분은 수식 정의이고, 수치 미분은 이를 근사치로 계산하는 것을 말한다.

근사치를 (x+h)-x 로 하는 전방차분이 있고, (x+h)-(x-h)로 하는 중심 차분 혹은 중앙 차분이 있다.

```python
def numerical_diff(f, x):
	h = 1e-4
    return (f(x+h) - f(x-h)) / (2*h) # 중심 차분
```

### 편미분, 전미분
편미분은 다변수 함수에서, 특정 변수를 제외한 나머지 변수를 상수로 생각하여 미분하는 것이다. 해당 변수(방향)에 대해서만 함수 값이 얼마나 변하는 지를 나타낸다.

![](https://images.velog.io/images/megachj/post/682a9df4-b5bb-4944-b976-36b88c9b2e0a/4-5.png)

전미분은 모든 변수(방향)에 대해서 전체 함수 값이 얼마나 변하는지를 나타낸다.
개별 변수가 변할 때, 변하는 값을 모두 더한것 만큼 변하게 되므로 아래 식이 된다.

![](https://images.velog.io/images/megachj/post/1567b7b7-968e-401c-967d-f2b89318d3a6/4-6.png)

## 기울기
모든 변수의 편미분을 `벡터`로 정리한 것을 기울기(gradient)라고 한다. 

![](https://images.velog.io/images/megachj/post/d345653e-655a-4b40-9c20-544ee46674b6/4-7.png)

기울기(gradient)는 벡터이므로 방향과 크기를 갖는데, 이때 방향은 해당 점에서 함수 값이 가장 커지는 방향이고, 크기는 함수 값의 변화량(전미분 값)이다. 

![](https://images.velog.io/images/megachj/post/df27976f-c9d3-4555-8ed1-bbc006e97dd9/4-8.png)

![](https://images.velog.io/images/megachj/post/9ce6410c-85ac-4439-a73f-74efa55d3d59/4-9.png)

파이썬으로는 기울기를 아래와 같이 구현할 수 있다.

```python
def numerical_gradient(f, x):
	h = 1e-4
    grad = np.zeros_like(x) # x와 형상이 같고 값은 모두 0인 배열
    
    for idx in range(x.size):
    	tmp_val = x[idx]
        
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 값 복원
    
    return grad
```

### 경사법(경사 하강법)
기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아내는 것이다. 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다. 여기서 최적이란 손실 함수의 최솟값일 때의 매개변수 값이다. 

이를 기울기를 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾아내는 것을 경사법이라 한다.

여기서 주의할 점은 각 점에서 함수 값을 낮추는 방안의 지표가 기울기인데, 이게 항상 최솟값으로 가는 방향이라고 보장하진 않는다. 왜냐하면 최솟값 뿐만 아니라 극솟값, 안장점(saddle point)에서도 기울기가 0이기 때문이다. 또 복잡하고 찌그러진 모양의 함수라면 대부분 평평한 곳으로 파고들면서 고원(plateau)이라 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다.

경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 이동한 곳에서도 동일하게 기울기를 구하고 기울어진 방향으로 가는 것을 반복한다. 

![](https://images.velog.io/images/megachj/post/e7c314b8-f681-4814-8409-1467fff1b98a/4-10.png)

학습률 값은 0.01, 0.001 등 미리 특정 값을 정해두어야 하는데, 일반적으로 너무 작거나 크면 좋은 장소를 찾아갈 수 없다.

파이썬으로 아래처럼 구현할 수 있다.
```python
# f: 최적화하려는 함수(손실 함수)
# lr: 학습률(learning rate)
def gradient_descent(f, init_x, lr=0.01, step_num=100):
	x = init_x
    
    for i in range(step_num):
    	grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
```

학습률 같은 매개변수를 `하이퍼파라미터`라고 한다. 신경망의 가중치, 편향은 학습 알고리즘에 의해 자동으로 획득되는 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수이다. 일반적으로 여러 후보 값 중에서 시험을 해보고 가장 잘 학습하는 값을 찾아야 한다.

### 신경망에서의 기울기
신경망 학습에서 기울기는 가중치(W) 매개변수에 대한 손실 함수의 기울기이다. m * n 형상인 가중치 W, 손실 함수 L인 신경망의 경사는 다음처럼 나타낼 수 있다.

![](https://images.velog.io/images/megachj/post/066294a1-e417-4350-832f-ac758b0e964e/4-11.png)

책에 나온 simpleNet 예제를 수행해보자.

## 학습 알고리즘 구현
신경망 학습의 절차는 다음과 같다.
```
# 전체
신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 4단계로 수행한다.

# 1단계: 미니배치
훈련 데이터 중 일부를 무작위로 가져온다. 선별된 데이터를 미니배치라 하며, 미니배치의 손실 함수 값을 줄이는 것이 목표이다.

# 2단계: 기울기 산출
미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.

# 3단계: 매개변수 갱신
가중치 매개변수를 기울기 방향으로 학습률만큼 갱신한다.

# 4단계: 반복
1~3단계를 반복한다.
```
이것이 신경망 학습 순서이며, 경사 하강법으로 매개변수를 갱신하는 방법이다. 이때 데이터를 미니배치로 무작위 선정하기 때문에 `확률적 경사 하강법(SGD: stochastic gradient descent)` 라고 부른다. 대부분 딥러닝 프레임워크는 `SGD`라는 함수로 이 기능을 구현하고 있다.

책의 예제처럼 MNIST 데이터셋을 사용해 학습을 수행해보자.